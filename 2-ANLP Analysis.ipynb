{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This .ipynb file is going to highlight some topics that is interesting for out project highliting what we learned in class and how we linked the knowledge with the small classification task we have in front of us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenges of the project are the same that we find in the NLP domain in general which are the Productivity, ambiguitty, variability, diversity and sparsity of language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2-TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diversity of the different languages makes the choice of the tokeniser a bit more difficult.We need a multilingual tokeniser that can deal with different languages and texts . The choice was to use BERT Multilingual (mBERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tonya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ceci', 'est', 'un', 'exemple', 'de', 'texte', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Charger le modèle et le tokenizer mBERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Tokenisation d'un exemple de texte\n",
    "example_text = \"Ceci est un exemple de texte.\"\n",
    "tokens = tokenizer.tokenize(example_text)\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some texts surpasses the number of 512 as we saw in the part 1-Exploratory Data Analysis .512 is the maximum number that the BERT multilingual model can work with.However we assumed that to guess a language 512 \"characters\" will be enough for our model to guess the language.This is why we decided to truncate all the texts/lines to 512 tokens each.For really small texts we decided to use paddings to also bring them to 512 tokens.This is what we call:text normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 190099 examples [00:04, 44019.51 examples/s]\n",
      "Map: 100%|██████████| 190099/190099 [10:19<00:00, 306.63 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Usage': 'Public', 'Text': 'َ قَالَ النَّبِيُّ ص إِنِّي أَتَعَجَّبُ مِمَّنْ يَضْرِبُ امْرَأَتَهُ وَ هُوَ بِالضَّرْبِ أَوْلَى مِنْهَا لَا تَضْرِبُوا نِسَاءَكُمْ بِالْخَشَبِ فَإِنَّ فِيهِ الْقِصَاصَ وَ لَكِنِ اضْرِبُوهُنَّ بِالْجُوعِ وَ الْعُرْيِ حَتَّى تُرِيحُوا [تَرْبَحُوا] فِي الدُّنْيَا وَ الْآخِرَةِ وَ أَيُّمَا رَجُلٍ تَتَزَيَّنُ امْرَأَتُهُ وَ تَخْرُجُ مِنْ بَابِ دَارِهَا فَهُوَ دَيُّوثٌ وَ لَا يَأْثَمُ مَنْ يُسَمِّيهِ دَيُّوثاً وَ الْمَرْأَةُ إِذَا خَرَجَتْ مِنْ بَابِ دَارِهَا مُتَزَيِّنَةً مُتَعَطِّرَةً وَ الزَّوْجُ بِذَلِكَ رَاضٍ يُبْنَى لِزَوْجِهَا بِكُلِّ قَدَمٍ بَيْتٌ فِي النَّارِ فَقَصِّرُوا أَجْنِحَةَ نِسَائِكُمْ وَ لَا تُطَوِّلُوهَا فَإِنَّ فِي تَقْصِيرِ أَجْنِحَتِهَا رِضًى وَ سُرُوراً وَ دُخُولَ الْجَنَّةِ بِغَيْرِ حِسَابٍ احْفَظُوا وَصِيَّتِي فِي أَمْرِ نِسَائِكُمْ حَتَّى تَنْجُوا مِنْ شِدَّةِ الْحِسَابِ وَ مَنْ لَمْ يَحْفَظْ وَصِيَّتِي فَمَا أَسْوَأَ حَالَهُ بَيْنَ يَدَيِ اللَّهِ وَ قَالَ ع النِّسَاءُ حَبَائِلُ الشَّيْطَان', 'Label': 1, 'text_length': 924, 'input_ids': [101, 797, 785, 47110, 10961, 23112, 59901, 10582, 45366, 11086, 93510, 40381, 18519, 777, 761, 101620, 21421, 18519, 10461, 759, 23112, 10502, 23112, 11693, 23112, 13027, 45366, 11086, 40381, 788, 21421, 10700, 45366, 10582, 51919, 793, 23112, 15386, 51919, 10673, 21421, 11086, 40381, 77111, 51919, 10673, 23112, 35849, 23112, 10502, 23112, 10388, 40381, 791, 23112, 790, 40381, 11145, 23112, 764, 21421, 13154, 15386, 45366, 10673, 51919, 11086, 21421, 759, 23112, 11145, 51919, 10961, 23112, 11832, 788, 101620, 51919, 10388, 47110, 787, 47110, 766, 23112, 15386, 51919, 10673, 21421, 11086, 40381, 14556, 789, 21421, 11091, 47110, 12611, 23112, 12497, 40381, 108668, 764, 21421, 13154, 51919, 16498, 23112, 11626, 23112, 11086, 21421, 784, 23112, 111171, 101620, 45366, 784, 93510, 10388, 21421, 59901, 51919, 11852, 21421, 15470, 47110, 15470, 23112, 791, 23112, 787, 23112, 12497, 101620, 21421, 763, 15386, 51919, 10673, 21421, 11086, 40381, 37019, 40381, 10582, 45366, 764, 21421, 13154, 51919, 13027, 40381, 23645, 21421, 791, 23112, 59901, 51919, 11693, 40381, 10673, 51919, 10461, 21421, 769, 23112, 10502, 45366, 11832, 766, 40381, 10673, 93510, 12616, 40381, 14556, 164, 766, 23112, 10673, 51919, 11086, 23112, 12616, 40381, 14556, 166, 784, 93510, 59901, 10658, 40381, 18519, 10582, 51919, 10461, 47110, 791, 23112, 59901, 51919, 111170, 16498, 21421, 10673, 23112, 10382, 21421, 791, 23112, 759, 23112, 10461, 40381, 18519, 10700, 47110, 773, 23112, 13027, 40381, 10961, 48406, 766, 23112, 10502, 23112, 11509, 23112, 10461, 45366, 10582, 40381, 77111, 51919, 10673, 23112, 35849, 23112, 10502, 40381, 10388, 40381, 791, 23112, 766, 23112, 16498, 51919, 10673, 40381, 13027, 40381, 788, 101620, 51919, 764, 47110, 11086, 21421, 771, 47110, 10673, 21421, 10388, 47110, 784, 23112, 10388, 40381, 11145, 23112, 771, 23112, 10461, 40381, 18519, 42116, 111172, 791, 23112, 787, 47110, 793, 23112, 35849, 51919, 15909, 23112, 10700, 40381, 788, 23112, 10582, 51919, 793, 40381, 11091, 23112, 10700, 21421, 18519, 23505, 21421, 771, 23112, 10461, 40381, 18519, 42116, 11870, 791, 23112, 59901, 51919, 10700, 23112, 10673, 51919, 35849, 23112, 10382, 40381, 761, 21421, 22973, 47110, 770, 23112, 10673, 23112, 13027, 23112, 10502, 51919, 788, 101620, 51919, 764, 47110, 11086, 21421, 771, 47110, 10673, 21421, 10388, 47110, 788, 40381, 10502, 23112, 11509, 23112, 10461, 21421, 18519, 10582, 23112, 10382, 16275, 788, 40381, 10502, 23112, 11693, 23112, 14286, 21421, 18519, 10673, 23112, 10382, 16275, 791, 23112, 59901, 11509, 45366, 11145, 51919, 13027, 40381, 764, 21421, 22973, 23112, 10961, 21421, 12497, 23112, 773, 47110, 15386, 48406, 793, 40381, 11086, 51919, 10582, 23112, 11832, 787, 21421, 11509, 23112, 11145, 51919, 13027, 21421, 10388, 47110, 764, 21421, 12497, 40381, 10961, 21421, 18519, 785, 23112, 10658, 23112, 10700, 48406, 764, 23112, 10461, 51919, 10502, 111172, 784, 93510, 59901, 10582, 45366, 11884, 21421, 784, 23112, 11852, 23112, 15470, 21421, 18519, 10673, 40381, 14556, 759, 23112, 13027, 51919, 10582, 21421, 12616, 23112, 10382, 23112, 789, 21421, 11091, 47110, 32345, 21421, 12497, 40381, 108668, 791, 23112, 787, 47110, 766, 40381, 14286, 23112, 11145, 21421, 82722, 40381, 37019, 47110, 784, 23112, 111171, 101620, 45366, 784, 93510, 766, 23112, 11852, 51919, 15470, 93510, 10673, 21421, 759, 23112, 13027, 51919, 10582, 21421, 12616, 23112, 10502, 21421, 10388, 47110, 773, 21421, 15386, 16275, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Charger le tokenizer BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Charger le fichier CSV avec `datasets`\n",
    "dataset = load_dataset('csv', data_files={'train': 'train_cleaned_numeric_labels.csv'}, delimiter=',')\n",
    "\n",
    "# Fonction de tokenisation avec tronquage et padding à une longueur maximale de 512 tokens\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Appliquer la tokenisation uniquement à la colonne 'Text' du dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Vérifier les premières entrées après tokenisation\n",
    "\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first intuition with the BERT tokeniser was not very fruitful.The tokeniser was not able to identify arabic words for exemple and was creatinng a unique token for all the sentence.We decided to try another tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 618, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1951, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/IreneETUDES/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/f9/9hrkdqhn469fr8hm8kzx8h5h0000gp/T/ipykernel_52097/434351785.py\", line 1, in <module>\n",
      "    from transformers import XLMRobertaTokenizer\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
      "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 39, in <module>\n",
      "    from torch import Tensor\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nXLMRobertaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Charger le tokenizer XLM-RoBERTa\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mXLMRobertaTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxlm-roberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Charger le dataset\u001b[39;00m\n\u001b[1;32m      8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m, data_files\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_cleaned_numeric_labels.csv\u001b[39m\u001b[38;5;124m'\u001b[39m}, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nXLMRobertaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Charger le tokenizer XLM-RoBERTa\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Charger le dataset\n",
    "dataset = load_dataset('csv', data_files={'train': 'train_cleaned_numeric_labels.csv'}, delimiter=',')\n",
    "\n",
    "# Afficher le premier texte du dataset pour vérification\n",
    "text = dataset['train']['Text'][0]\n",
    "print(\"Texte original :\")\n",
    "print(text)\n",
    "\n",
    "# Tokenisation manuelle du premier texte\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"\\nTokens générés :\")\n",
    "print(tokens)\n",
    "\n",
    "# Fonction de tokenisation pour le dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['Text'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"  # Retourne des tenseurs PyTorch (optionnel)\n",
    "    )\n",
    "\n",
    "# Appliquer la tokenisation à l'ensemble du dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, batch_size=1000)\n",
    "\n",
    "# Afficher les résultats de la tokenisation pour le premier exemple\n",
    "print(\"\\nIDs des tokens pour le premier exemple :\")\n",
    "print(tokenized_datasets['train'][0]['input_ids'])\n",
    "\n",
    "print(\"\\nMasque d'attention pour le premier exemple :\")\n",
    "print(tokenized_datasets['train'][0]['attention_mask'])\n",
    "\n",
    "# Optionnel : Convertir les IDs en tokens pour vérification\n",
    "input_ids = tokenized_datasets['train'][0]['input_ids']\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(\"\\nTokens reconstruits à partir des IDs :\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step was not mandatory, but the problem is with languages such as arabic,chinese or tukish where the characters are with a complex morphology and the words are not seperated, tokenizinng simplify the problem a lot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
