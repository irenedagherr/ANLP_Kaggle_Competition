{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This .ipynb file is going to highlight some topics that is interesting for out project highliting what we learned in class and how we linked the knowledge with the small classification task we have in front of us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenges of the project are the same that we find in the NLP domain in general which are the Productivity, ambiguitty, variability, diversity and sparsity of language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diversity of the different languages makes the choice of the tokeniser a bit more difficult.We need a multilingual tokeniser that can deal with different languages and texts . The choice was to use BERT Multilingual (mBERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tonya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ceci', 'est', 'un', 'exemple', 'de', 'texte', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Charger le modèle et le tokenizer mBERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Tokenisation d'un exemple de texte\n",
    "example_text = \"Ceci est un exemple de texte.\"\n",
    "tokens = tokenizer.tokenize(example_text)\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some texts surpasses the number of 512 as we saw in the part 1-Exploratory Data Analysis .512 is the maximum number that the BERT multilingual model can work with.However we assumed that to guess a language 512 \"characters\" will be enough for our model to guess the language.This is why we decided to truncate all the texts/lines to 512 tokens each.For really small texts we decided to use paddings to also bring them to 512 tokens.This is what we call:text normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 190099 examples [00:04, 44019.51 examples/s]\n",
      "Map: 100%|██████████| 190099/190099 [10:19<00:00, 306.63 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Usage': 'Public', 'Text': 'َ قَالَ النَّبِيُّ ص إِنِّي أَتَعَجَّبُ مِمَّنْ يَضْرِبُ امْرَأَتَهُ وَ هُوَ بِالضَّرْبِ أَوْلَى مِنْهَا لَا تَضْرِبُوا نِسَاءَكُمْ بِالْخَشَبِ فَإِنَّ فِيهِ الْقِصَاصَ وَ لَكِنِ اضْرِبُوهُنَّ بِالْجُوعِ وَ الْعُرْيِ حَتَّى تُرِيحُوا [تَرْبَحُوا] فِي الدُّنْيَا وَ الْآخِرَةِ وَ أَيُّمَا رَجُلٍ تَتَزَيَّنُ امْرَأَتُهُ وَ تَخْرُجُ مِنْ بَابِ دَارِهَا فَهُوَ دَيُّوثٌ وَ لَا يَأْثَمُ مَنْ يُسَمِّيهِ دَيُّوثاً وَ الْمَرْأَةُ إِذَا خَرَجَتْ مِنْ بَابِ دَارِهَا مُتَزَيِّنَةً مُتَعَطِّرَةً وَ الزَّوْجُ بِذَلِكَ رَاضٍ يُبْنَى لِزَوْجِهَا بِكُلِّ قَدَمٍ بَيْتٌ فِي النَّارِ فَقَصِّرُوا أَجْنِحَةَ نِسَائِكُمْ وَ لَا تُطَوِّلُوهَا فَإِنَّ فِي تَقْصِيرِ أَجْنِحَتِهَا رِضًى وَ سُرُوراً وَ دُخُولَ الْجَنَّةِ بِغَيْرِ حِسَابٍ احْفَظُوا وَصِيَّتِي فِي أَمْرِ نِسَائِكُمْ حَتَّى تَنْجُوا مِنْ شِدَّةِ الْحِسَابِ وَ مَنْ لَمْ يَحْفَظْ وَصِيَّتِي فَمَا أَسْوَأَ حَالَهُ بَيْنَ يَدَيِ اللَّهِ وَ قَالَ ع النِّسَاءُ حَبَائِلُ الشَّيْطَان', 'Label': 1, 'text_length': 924, 'input_ids': [101, 797, 785, 47110, 10961, 23112, 59901, 10582, 45366, 11086, 93510, 40381, 18519, 777, 761, 101620, 21421, 18519, 10461, 759, 23112, 10502, 23112, 11693, 23112, 13027, 45366, 11086, 40381, 788, 21421, 10700, 45366, 10582, 51919, 793, 23112, 15386, 51919, 10673, 21421, 11086, 40381, 77111, 51919, 10673, 23112, 35849, 23112, 10502, 23112, 10388, 40381, 791, 23112, 790, 40381, 11145, 23112, 764, 21421, 13154, 15386, 45366, 10673, 51919, 11086, 21421, 759, 23112, 11145, 51919, 10961, 23112, 11832, 788, 101620, 51919, 10388, 47110, 787, 47110, 766, 23112, 15386, 51919, 10673, 21421, 11086, 40381, 14556, 789, 21421, 11091, 47110, 12611, 23112, 12497, 40381, 108668, 764, 21421, 13154, 51919, 16498, 23112, 11626, 23112, 11086, 21421, 784, 23112, 111171, 101620, 45366, 784, 93510, 10388, 21421, 59901, 51919, 11852, 21421, 15470, 47110, 15470, 23112, 791, 23112, 787, 23112, 12497, 101620, 21421, 763, 15386, 51919, 10673, 21421, 11086, 40381, 37019, 40381, 10582, 45366, 764, 21421, 13154, 51919, 13027, 40381, 23645, 21421, 791, 23112, 59901, 51919, 11693, 40381, 10673, 51919, 10461, 21421, 769, 23112, 10502, 45366, 11832, 766, 40381, 10673, 93510, 12616, 40381, 14556, 164, 766, 23112, 10673, 51919, 11086, 23112, 12616, 40381, 14556, 166, 784, 93510, 59901, 10658, 40381, 18519, 10582, 51919, 10461, 47110, 791, 23112, 59901, 51919, 111170, 16498, 21421, 10673, 23112, 10382, 21421, 791, 23112, 759, 23112, 10461, 40381, 18519, 10700, 47110, 773, 23112, 13027, 40381, 10961, 48406, 766, 23112, 10502, 23112, 11509, 23112, 10461, 45366, 10582, 40381, 77111, 51919, 10673, 23112, 35849, 23112, 10502, 40381, 10388, 40381, 791, 23112, 766, 23112, 16498, 51919, 10673, 40381, 13027, 40381, 788, 101620, 51919, 764, 47110, 11086, 21421, 771, 47110, 10673, 21421, 10388, 47110, 784, 23112, 10388, 40381, 11145, 23112, 771, 23112, 10461, 40381, 18519, 42116, 111172, 791, 23112, 787, 47110, 793, 23112, 35849, 51919, 15909, 23112, 10700, 40381, 788, 23112, 10582, 51919, 793, 40381, 11091, 23112, 10700, 21421, 18519, 23505, 21421, 771, 23112, 10461, 40381, 18519, 42116, 11870, 791, 23112, 59901, 51919, 10700, 23112, 10673, 51919, 35849, 23112, 10382, 40381, 761, 21421, 22973, 47110, 770, 23112, 10673, 23112, 13027, 23112, 10502, 51919, 788, 101620, 51919, 764, 47110, 11086, 21421, 771, 47110, 10673, 21421, 10388, 47110, 788, 40381, 10502, 23112, 11509, 23112, 10461, 21421, 18519, 10582, 23112, 10382, 16275, 788, 40381, 10502, 23112, 11693, 23112, 14286, 21421, 18519, 10673, 23112, 10382, 16275, 791, 23112, 59901, 11509, 45366, 11145, 51919, 13027, 40381, 764, 21421, 22973, 23112, 10961, 21421, 12497, 23112, 773, 47110, 15386, 48406, 793, 40381, 11086, 51919, 10582, 23112, 11832, 787, 21421, 11509, 23112, 11145, 51919, 13027, 21421, 10388, 47110, 764, 21421, 12497, 40381, 10961, 21421, 18519, 785, 23112, 10658, 23112, 10700, 48406, 764, 23112, 10461, 51919, 10502, 111172, 784, 93510, 59901, 10582, 45366, 11884, 21421, 784, 23112, 11852, 23112, 15470, 21421, 18519, 10673, 40381, 14556, 759, 23112, 13027, 51919, 10582, 21421, 12616, 23112, 10382, 23112, 789, 21421, 11091, 47110, 32345, 21421, 12497, 40381, 108668, 791, 23112, 787, 47110, 766, 40381, 14286, 23112, 11145, 21421, 82722, 40381, 37019, 47110, 784, 23112, 111171, 101620, 45366, 784, 93510, 766, 23112, 11852, 51919, 15470, 93510, 10673, 21421, 759, 23112, 13027, 51919, 10582, 21421, 12616, 23112, 10502, 21421, 10388, 47110, 773, 21421, 15386, 16275, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Charger le tokenizer BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Charger le fichier CSV avec `datasets`\n",
    "dataset = load_dataset('csv', data_files={'train': 'train_cleaned_numeric_labels.csv'}, delimiter=',')\n",
    "\n",
    "# Fonction de tokenisation avec tronquage et padding à une longueur maximale de 512 tokens\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Appliquer la tokenisation uniquement à la colonne 'Text' du dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Vérifier les premières entrées après tokenisation\n",
    "\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
