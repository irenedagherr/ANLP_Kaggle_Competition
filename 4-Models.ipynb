{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "When choosing a model two approaches were given to us.Either creating our own model using TF-IDF , or using an already made model like BERT, ROBERTA etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model below is a model using a TF-IDF Vectorizer and Naive Bayes‚Äîa classic approach.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4-TF IDF AND NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Charger les donn√©es\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"train_cleaned_numeric_labels.csv\")\n",
    "\n",
    "# Transformer les textes en vecteurs TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "X_tfidf = vectorizer.fit_transform(df[\"Text\"])\n",
    "\n",
    "# S√©parer les donn√©es en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df[\"Label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Entra√Æner le mod√®le Naive Bayes\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "# Pr√©dictions\n",
    "y_pred = model_nb.predict(X_test)\n",
    "\n",
    "# √âvaluer l'accuracy\n",
    "print(f\"Accuracy du mod√®le Naive Bayes : {accuracy_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Charger les donn√©es\n",
    "df = pd.read_csv(\"train_cleaned_numeric_labels.csv\")\n",
    "\n",
    "# Tokenisation des textes\n",
    "tokenizer = Tokenizer(num_words=50000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"Text\"])\n",
    "X_seq = tokenizer.texts_to_sequences(df[\"Text\"])\n",
    "\n",
    "# Padding des s√©quences\n",
    "X_pad = pad_sequences(X_seq, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# S√©parer les donn√©es en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, df[\"Label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Cr√©ation du mod√®le LSTM\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=50000, output_dim=128, input_length=200),\n",
    "    SpatialDropout1D(0.2),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(len(df[\"Label\"].unique()), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compilation du mod√®le\n",
    "model_lstm.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Entra√Ænement\n",
    "model_lstm.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# √âvaluation\n",
    "loss, acc = model_lstm.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy du mod√®le LSTM : {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5-Pre-existing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is our first attempt at using models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Charger le tokenizer et le mod√®le BERT multilingue\n",
    "model_name = 'bert-base-multilingual-cased'  # ou 'xlm-roberta-base' pour XLM-RoBERTa\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=386)  # 386 labels pour tes 386 langues\n",
    "\n",
    "# Charger le dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"train_cleaned_numeric_labels.csv\")\n",
    "\n",
    "# Tokenisation des textes\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "# Appliquer la tokenisation au dataset\n",
    "tokenized_dataset = df[['Text', 'Label']].rename(columns={'Text': 'text', 'Label': 'label'})\n",
    "tokenized_dataset = tokenized_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Diviser le dataset en train/test\n",
    "train_dataset, test_dataset = train_test_split(tokenized_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir en Dataset Hugging Face\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_dataset)\n",
    "test_dataset = Dataset.from_pandas(test_dataset)\n",
    "\n",
    "# D√©finir les arguments pour l'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    evaluation_strategy=\"epoch\",     \n",
    "    learning_rate=2e-5,              \n",
    "    per_device_train_batch_size=8,  \n",
    "    per_device_eval_batch_size=8,   \n",
    "    num_train_epochs=3,             \n",
    "    weight_decay=0.01,              \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=200,\n",
    ")\n",
    "\n",
    "# Fonction d'√©valuation pour calculer l'accuracy\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = torch.argmax(predictions, axis=1)\n",
    "    return {'accuracy': accuracy_score(labels, preds)}\n",
    "\n",
    "# Cr√©er le Trainer Hugging Face\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,            \n",
    "    compute_metrics=compute_metrics       \n",
    ")\n",
    "\n",
    "# Entra√Æner le mod√®le\n",
    "trainer.train()\n",
    "\n",
    "# √âvaluer le mod√®le\n",
    "results = trainer.evaluate()\n",
    "print(f\"Accuracy sur le dataset de test : {results['eval_accuracy']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE CODE WITH AN ACCURACY OF 0.73"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of our code is in the .ipynb named : Models.ipynb (we ran it on another file so in order to see the results we needed to split it ). However,the code below is the explanation of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch scikit-learn pandas numpy tqdm accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Configuration du GPU (s'il est disponible)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Chargement des donn√©es\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# Nettoyage des textes\n",
    "def clean_text(text, max_length=256):  # ‚úÖ R√©duction √† 256 tokens\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# Filtrer les classes qui ont au moins 2 √©chantillons\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 2].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# R√©duction du dataset (max 25 000 exemples)\n",
    "max_samples = 25000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# Chargement du mod√®le RoBERTa \n",
    "model_name = \"roberta-large\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_id)).to(device)\n",
    "\n",
    "# Activation du `gradient_checkpointing` (just to save the model)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# D√©finition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):  #  max_length optimis√©\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Cr√©ation des DataLoaders\n",
    "batch_size = 32  #  Optimis√© pour GPU\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Optimisation :\n",
    "learning_rate = 2e-5  \n",
    "epochs = 5  \n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=1000, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "#  Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "#  Entra√Ænement \n",
    "print(\"üöÄ Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(): \n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\" Training Complete\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
