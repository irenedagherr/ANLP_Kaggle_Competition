{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "When choosing a model two approaches were given to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Charger les données\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"train_cleaned_numeric_labels.csv\")\n",
    "\n",
    "# Transformer les textes en vecteurs TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "X_tfidf = vectorizer.fit_transform(df[\"Text\"])\n",
    "\n",
    "# Séparer les données en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df[\"Label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraîner le modèle Naive Bayes\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions\n",
    "y_pred = model_nb.predict(X_test)\n",
    "\n",
    "# Évaluer l'accuracy\n",
    "print(f\"Accuracy du modèle Naive Bayes : {accuracy_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv(\"train_cleaned_numeric_labels.csv\")\n",
    "\n",
    "# Tokenisation des textes\n",
    "tokenizer = Tokenizer(num_words=50000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"Text\"])\n",
    "X_seq = tokenizer.texts_to_sequences(df[\"Text\"])\n",
    "\n",
    "# Padding des séquences\n",
    "X_pad = pad_sequences(X_seq, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# Séparer les données en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, df[\"Label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Création du modèle LSTM\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=50000, output_dim=128, input_length=200),\n",
    "    SpatialDropout1D(0.2),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(len(df[\"Label\"].unique()), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compilation du modèle\n",
    "model_lstm.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Entraînement\n",
    "model_lstm.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Évaluation\n",
    "loss, acc = model_lstm.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy du modèle LSTM : {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
