{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "When choosing a model two approaches were given to us.Either creating our own model using TF-IDF , or using an already made model like BERT, ROBERTA etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model below is a model using a TF-IDF Vectorizer and Naive Bayes—a classic approach.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4-TF IDF AND NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Charger les données\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"train_cleaned_numeric_labels.csv\")\n",
    "\n",
    "# Transformer les textes en vecteurs TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "X_tfidf = vectorizer.fit_transform(df[\"Text\"])\n",
    "\n",
    "# Séparer les données en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df[\"Label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraîner le modèle Naive Bayes\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions\n",
    "y_pred = model_nb.predict(X_test)\n",
    "\n",
    "# Évaluer l'accuracy\n",
    "print(f\"Accuracy du modèle Naive Bayes : {accuracy_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv(\"train_cleaned_numeric_labels.csv\")\n",
    "\n",
    "# Tokenisation des textes\n",
    "tokenizer = Tokenizer(num_words=50000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"Text\"])\n",
    "X_seq = tokenizer.texts_to_sequences(df[\"Text\"])\n",
    "\n",
    "# Padding des séquences\n",
    "X_pad = pad_sequences(X_seq, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# Séparer les données en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, df[\"Label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Création du modèle LSTM\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=50000, output_dim=128, input_length=200),\n",
    "    SpatialDropout1D(0.2),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(len(df[\"Label\"].unique()), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compilation du modèle\n",
    "model_lstm.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Entraînement\n",
    "model_lstm.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Évaluation\n",
    "loss, acc = model_lstm.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy du modèle LSTM : {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5-Pre-existing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is our first attempt at using models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Charger le tokenizer et le modèle BERT multilingue\n",
    "model_name = 'bert-base-multilingual-cased'  # ou 'xlm-roberta-base' pour XLM-RoBERTa\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=386)  # 386 labels pour tes 386 langues\n",
    "\n",
    "# Charger le dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"train_cleaned_numeric_labels.csv\")\n",
    "\n",
    "# Tokenisation des textes\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "# Appliquer la tokenisation au dataset\n",
    "tokenized_dataset = df[['Text', 'Label']].rename(columns={'Text': 'text', 'Label': 'label'})\n",
    "tokenized_dataset = tokenized_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Diviser le dataset en train/test\n",
    "train_dataset, test_dataset = train_test_split(tokenized_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir en Dataset Hugging Face\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_dataset)\n",
    "test_dataset = Dataset.from_pandas(test_dataset)\n",
    "\n",
    "# Définir les arguments pour l'entraînement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    evaluation_strategy=\"epoch\",     \n",
    "    learning_rate=2e-5,              \n",
    "    per_device_train_batch_size=8,  \n",
    "    per_device_eval_batch_size=8,   \n",
    "    num_train_epochs=3,             \n",
    "    weight_decay=0.01,              \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=200,\n",
    ")\n",
    "\n",
    "# Fonction d'évaluation pour calculer l'accuracy\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = torch.argmax(predictions, axis=1)\n",
    "    return {'accuracy': accuracy_score(labels, preds)}\n",
    "\n",
    "# Créer le Trainer Hugging Face\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,            \n",
    "    compute_metrics=compute_metrics       \n",
    ")\n",
    "\n",
    "# Entraîner le modèle\n",
    "trainer.train()\n",
    "\n",
    "# Évaluer le modèle\n",
    "results = trainer.evaluate()\n",
    "print(f\"Accuracy sur le dataset de test : {results['eval_accuracy']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE CODE WITH AN ACCURACY OF 0.73"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of our code is in the .ipynb named : Models.ipynb (we ran it on another file so in order to see the results we needed to split it ). However,the code below is the explanation of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
