{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763e1485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\r\n",
      "Built on Wed_Jun__8_16:49:14_PDT_2022\r\n",
      "Cuda compilation tools, release 11.7, V11.7.99\r\n",
      "Build cuda_11.7.r11.7/compiler.31442593_0\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d8542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/airbuscomputervision/abifarah_nan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n",
      "🚀 Loading distilbert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:92: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 1/7: 100%|██████████| 663/663 [01:22<00:00,  7.99it/s, loss=4.3890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 - Avg Training Loss: 5.4645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 2/7: 100%|██████████| 663/663 [01:26<00:00,  7.66it/s, loss=2.7561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 - Avg Training Loss: 3.5443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 3/7: 100%|██████████| 663/663 [01:27<00:00,  7.57it/s, loss=1.8300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 - Avg Training Loss: 2.4533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 4/7: 100%|██████████| 663/663 [01:28<00:00,  7.51it/s, loss=1.8888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 - Avg Training Loss: 1.9162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 5/7: 100%|██████████| 663/663 [01:28<00:00,  7.47it/s, loss=1.6611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 - Avg Training Loss: 1.6036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 6/7: 100%|██████████| 663/663 [01:29<00:00,  7.44it/s, loss=1.3162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 - Avg Training Loss: 1.4190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 7/7: 100%|██████████| 663/663 [01:29<00:00,  7.44it/s, loss=1.0920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 - Avg Training Loss: 1.3199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00,  9.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6387\n",
      "✅ Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 📌 Installer les packages nécessaires si besoin\n",
    "# !pip install transformers datasets torch scikit-learn pandas numpy tqdm accelerate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# ✅ Détection du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# ✅ Chargement des données\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# ✅ Nettoyage des textes\n",
    "def clean_text(text, max_length=256):  # ✅ Réduction à 256 tokens pour accélérer\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# ✅ Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# ✅ Filtrer les classes qui ont au moins 5 échantillons\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 5].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Réduction du dataset (max 25 000 exemples pour rapidité)\n",
    "max_samples = 25000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# ✅ Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Chargement du modèle DistilBERT (Rapide et performant)\n",
    "model_name = \"distilbert-base-uncased\"  # 📌 Plus rapide que DeBERTa\n",
    "print(f\"🚀 Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_id)).to(device)\n",
    "\n",
    "# ✅ Définition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):  # ✅ max_length optimisé\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ✅ Création des DataLoaders\n",
    "batch_size = 32  # ✅ Batch optimisé pour rapidité\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ✅ Optimisation : AdamW + Scheduler\n",
    "learning_rate = 2e-5  # ✅ LR ajusté pour rapidité\n",
    "epochs = 7  # ✅ Réduction des époques pour accélérer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=500, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "# ✅ Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ✅ Entraînement (Optimisé)\n",
    "print(\"🚀 Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # ✅ Mixed Precision Training\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # ✅ Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\"✅ Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2e42fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/airbuscomputervision/abifarah_nan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:93: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 1/10: 100%|██████████| 664/664 [01:54<00:00,  5.79it/s, loss=4.8026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Avg Training Loss: 5.4961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 2/10: 100%|██████████| 664/664 [01:46<00:00,  6.23it/s, loss=3.1351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Avg Training Loss: 3.8583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 3/10: 100%|██████████| 664/664 [01:47<00:00,  6.19it/s, loss=1.9742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Avg Training Loss: 2.5508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 4/10: 100%|██████████| 664/664 [01:46<00:00,  6.21it/s, loss=1.2819]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Avg Training Loss: 1.8354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 5/10: 100%|██████████| 664/664 [01:46<00:00,  6.21it/s, loss=1.5340]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Avg Training Loss: 1.4243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 6/10: 100%|██████████| 664/664 [01:47<00:00,  6.21it/s, loss=0.8184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Avg Training Loss: 1.1545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 7/10: 100%|██████████| 664/664 [01:47<00:00,  6.21it/s, loss=0.9530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Avg Training Loss: 0.9738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 8/10: 100%|██████████| 664/664 [01:47<00:00,  6.20it/s, loss=0.5812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Avg Training Loss: 0.8431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 9/10: 100%|██████████| 664/664 [01:47<00:00,  6.19it/s, loss=0.9668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Avg Training Loss: 0.7561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 10/10: 100%|██████████| 664/664 [01:47<00:00,  6.20it/s, loss=0.7178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Avg Training Loss: 0.7026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7364\n",
      "✅ Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 📌 Installer les packages nécessaires si besoin\n",
    "# !pip install transformers datasets torch scikit-learn pandas numpy tqdm accelerate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# ✅ Détection du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# ✅ Chargement des données\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# ✅ Nettoyage des textes\n",
    "def clean_text(text, max_length=256):  # ✅ Réduction à 256 tokens\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# ✅ Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# ✅ Filtrer les classes qui ont au moins 2 échantillons\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 2].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Réduction du dataset (max 25 000 exemples)\n",
    "max_samples = 25000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# ✅ Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Chargement du modèle RoBERTa (Meilleur que DistilBERT)\n",
    "model_name = \"roberta-base\"  # 📌 Plus puissant que DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_id)).to(device)\n",
    "\n",
    "# ✅ Activation du `gradient_checkpointing` (économie de mémoire)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# ✅ Définition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):  # ✅ max_length optimisé\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ✅ Création des DataLoaders\n",
    "batch_size = 32  # ✅ Optimisé pour GPU\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ✅ Optimisation : AdamW + Scheduler\n",
    "learning_rate = 2e-5  # ✅ LR plus bas pour une meilleure convergence\n",
    "epochs = 10  # ✅ Augmenté pour une meilleure précision\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=1000, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "# ✅ Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ✅ Entraînement (Optimisé)\n",
    "print(\"🚀 Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # ✅ Mixed Precision Training\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # ✅ Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\"✅ Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786d8fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/airbuscomputervision/abifarah_nan/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:93: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 1/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=4.7588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Avg Training Loss: 5.6270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 2/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=2.4141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Avg Training Loss: 3.2152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:31<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 3/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=0.9851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Avg Training Loss: 1.5922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 4/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=0.9995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Avg Training Loss: 1.0448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 5/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=0.4428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Avg Training Loss: 0.7092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 6/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=0.3385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Avg Training Loss: 0.5267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 7/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=0.4267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Avg Training Loss: 0.4022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 8/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=0.4626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Avg Training Loss: 0.3156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 9/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=0.1822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Avg Training Loss: 0.2421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:31<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 10/10:  34%|███▍      | 226/664 [03:02<05:53,  1.24it/s, loss=0.3311]"
     ]
    }
   ],
   "source": [
    "# 📌 Installer les packages nécessaires si besoin\n",
    "# !pip install transformers datasets torch scikit-learn pandas numpy tqdm accelerate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# ✅ Détection du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# ✅ Chargement des données\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# ✅ Nettoyage des textes\n",
    "def clean_text(text, max_length=256):  # ✅ Réduction à 256 tokens\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# ✅ Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# ✅ Filtrer les classes qui ont au moins 2 échantillons\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 2].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Réduction du dataset (max 25 000 exemples)\n",
    "max_samples = 25000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# ✅ Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Chargement du modèle RoBERTa (Meilleur que DistilBERT)\n",
    "model_name = \"microsoft/deberta-v3-large\"  # 📌 Plus puissant que DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_id)).to(device)\n",
    "\n",
    "# ✅ Activation du `gradient_checkpointing` (économie de mémoire)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# ✅ Définition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):  # ✅ max_length optimisé\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ✅ Création des DataLoaders\n",
    "batch_size = 32  # ✅ Optimisé pour GPU\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ✅ Optimisation : AdamW + Scheduler\n",
    "learning_rate = 2e-5  # ✅ LR plus bas pour une meilleure convergence\n",
    "epochs = 10  # ✅ Augmenté pour une meilleure précision\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=1000, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "# ✅ Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ✅ Entraînement (Optimisé)\n",
    "print(\"🚀 Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # ✅ Mixed Precision Training\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # ✅ Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\"✅ Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fded11d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 5956/5956 [05:10<00:00, 19.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission file 'submission_bert.csv' generated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Process test data\n",
    "print(\"Processing test data...\")\n",
    "test_path = \"test_without_labels.csv\"\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Clean test data\n",
    "test_df[\"Cleaned_Text\"] = test_df[\"Text\"].apply(clean_text)\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, \n",
    "                                  max_length=max_length, return_tensors=\"pt\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "test_dataset = TestDataset(test_df[\"Cleaned_Text\"], tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Generate predictions\n",
    "model.eval()\n",
    "all_test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Generating predictions\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        # Get predictions\n",
    "        preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        all_test_preds.extend(preds)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "predicted_labels = [id_to_label[pred_id] for pred_id in all_test_preds]\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df.index + 1,  # Start IDs from 1\n",
    "    \"Label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_path = \"submission_bert.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"✅ Submission file '{submission_path}' generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e48cdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n",
      "Nombre total d'exemples après filtrage : 190087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:103: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/10: 100%|██████████| 531/531 [00:35<00:00, 14.88it/s, loss=2.8405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Avg Training Loss: 2.9818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:03<00:00, 19.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 2/10: 100%|██████████| 531/531 [00:36<00:00, 14.39it/s, loss=2.4766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Avg Training Loss: 2.5694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:03<00:00, 19.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 3/10: 100%|██████████| 531/531 [00:36<00:00, 14.40it/s, loss=2.4141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Avg Training Loss: 2.0705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:02<00:00, 19.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 4/10: 100%|██████████| 531/531 [00:36<00:00, 14.42it/s, loss=2.0706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Avg Training Loss: 1.6889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:02<00:00, 19.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 5/10: 100%|██████████| 531/531 [00:36<00:00, 14.41it/s, loss=1.5877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Avg Training Loss: 1.3878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:03<00:00, 19.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 6/10: 100%|██████████| 531/531 [00:37<00:00, 14.35it/s, loss=0.3924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Avg Training Loss: 1.1640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:02<00:00, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 7/10: 100%|██████████| 531/531 [00:36<00:00, 14.46it/s, loss=0.6785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Avg Training Loss: 1.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:03<00:00, 19.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 8/10: 100%|██████████| 531/531 [00:37<00:00, 14.30it/s, loss=0.6249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Avg Training Loss: 0.8795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:02<00:00, 19.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 9/10: 100%|██████████| 531/531 [00:36<00:00, 14.43it/s, loss=1.2419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Avg Training Loss: 0.7849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:02<00:00, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 10/10: 100%|██████████| 531/531 [00:36<00:00, 14.56it/s, loss=0.8906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Avg Training Loss: 0.7077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:02<00:00, 20.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6403\n",
      "✅ Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # ✅ Accélération supplémentaire\n",
    "\n",
    "# ✅ Détection du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# ✅ Chargement des données\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# ✅ Nettoyage des textes\n",
    "def clean_text(text, max_length=128):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# ✅ Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# ✅ Filtrer les classes qui ont au moins 3 échantillons (moins strict)\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 3].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Vérifier la taille du dataset après filtrage\n",
    "print(f\"Nombre total d'exemples après filtrage : {len(train_df)}\")\n",
    "\n",
    "# ✅ Réduction du dataset (max 20 000 exemples pour vitesse)\n",
    "max_samples = 20000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# ✅ Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Chargement du modèle **correctement initialisé**\n",
    "model_name = \"distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label_to_id)\n",
    ").to(device)\n",
    "\n",
    "# ✅ Réinitialiser les poids de classification\n",
    "torch.nn.init.xavier_uniform_(model.classifier.out_proj.weight)\n",
    "torch.nn.init.zeros_(model.classifier.out_proj.bias)\n",
    "\n",
    "# ✅ Activation de `gradient_checkpointing`\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# ✅ Définition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ✅ Création des DataLoaders\n",
    "batch_size = 32  # ✅ Plus stable\n",
    "gradient_accumulation_steps = 2  # ✅ Accumulation pour économiser la mémoire GPU\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ✅ Optimisation : AdamW + Scheduler\n",
    "learning_rate = 2e-5  # ✅ Learning rate plus stable\n",
    "epochs = 10  # ✅ Légèrement augmenté pour convergence\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=1000, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "# ✅ Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ✅ Entraînement optimisé (1h max)\n",
    "print(\"🚀 Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / gradient_accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # ✅ Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\"✅ Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c928642c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/airbuscomputervision/abifarah_nan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:93: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 1/5: 100%|██████████| 664/664 [09:00<00:00,  1.23it/s, loss=4.0703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Avg Training Loss: 5.1601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:48<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 2/5: 100%|██████████| 664/664 [09:02<00:00,  1.22it/s, loss=2.1680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Avg Training Loss: 2.7878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:49<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 3/5: 100%|██████████| 664/664 [09:02<00:00,  1.22it/s, loss=1.4139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Avg Training Loss: 1.5903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:50<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 4/5: 100%|██████████| 664/664 [09:02<00:00,  1.22it/s, loss=1.0661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Avg Training Loss: 1.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:50<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 5/5: 100%|██████████| 664/664 [09:03<00:00,  1.22it/s, loss=1.3816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Avg Training Loss: 0.8968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:49<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7246\n",
      "✅ Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 📌 Installer les packages nécessaires si besoin\n",
    "# !pip install transformers datasets torch scikit-learn pandas numpy tqdm accelerate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# ✅ Détection du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# ✅ Chargement des données\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# ✅ Nettoyage des textes\n",
    "def clean_text(text, max_length=256):  # ✅ Réduction à 256 tokens\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# ✅ Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# ✅ Filtrer les classes qui ont au moins 2 échantillons\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 2].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Réduction du dataset (max 25 000 exemples)\n",
    "max_samples = 25000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# ✅ Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Chargement du modèle RoBERTa (Meilleur que DistilBERT)\n",
    "model_name = \"roberta-large\"  # 📌 Plus puissant que DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_id)).to(device)\n",
    "\n",
    "# ✅ Activation du `gradient_checkpointing` (économie de mémoire)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# ✅ Définition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):  # ✅ max_length optimisé\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ✅ Création des DataLoaders\n",
    "batch_size = 32  # ✅ Optimisé pour GPU\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ✅ Optimisation : AdamW + Scheduler\n",
    "learning_rate = 2e-5  # ✅ LR plus bas pour une meilleure convergence\n",
    "epochs = 5  # ✅ Augmenté pour une meilleure précision\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=1000, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "# ✅ Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ✅ Entraînement (Optimisé)\n",
    "print(\"🚀 Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # ✅ Mixed Precision Training\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # ✅ Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\"✅ Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8286ef57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|█████████▉| 5949/5956 [35:26<00:02,  2.83it/s]"
     ]
    }
   ],
   "source": [
    "# Process test data\n",
    "print(\"Processing test data...\")\n",
    "test_path = \"test_without_labels.csv\"\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Clean test data\n",
    "test_df[\"Cleaned_Text\"] = test_df[\"Text\"].apply(clean_text)\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, \n",
    "                                  max_length=max_length, return_tensors=\"pt\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "test_dataset = TestDataset(test_df[\"Cleaned_Text\"], tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Generate predictions\n",
    "model.eval()\n",
    "all_test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Generating predictions\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        # Get predictions\n",
    "        preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        all_test_preds.extend(preds)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "predicted_labels = [id_to_label[pred_id] for pred_id in all_test_preds]\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df.index + 1,  # Start IDs from 1\n",
    "    \"Label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_path = \"submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"✅ Submission file '{submission_path}' generated successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mon_env)",
   "language": "python",
   "name": "mon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
