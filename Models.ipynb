{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763e1485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\r\n",
      "Built on Wed_Jun__8_16:49:14_PDT_2022\r\n",
      "Cuda compilation tools, release 11.7, V11.7.99\r\n",
      "Build cuda_11.7.r11.7/compiler.31442593_0\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d8542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/airbuscomputervision/abifarah_nan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n",
      "🚀 Loading distilbert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:92: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 1/7: 100%|██████████| 663/663 [01:22<00:00,  7.99it/s, loss=4.3890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 - Avg Training Loss: 5.4645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 2/7: 100%|██████████| 663/663 [01:26<00:00,  7.66it/s, loss=2.7561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 - Avg Training Loss: 3.5443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 3/7: 100%|██████████| 663/663 [01:27<00:00,  7.57it/s, loss=1.8300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 - Avg Training Loss: 2.4533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 4/7: 100%|██████████| 663/663 [01:28<00:00,  7.51it/s, loss=1.8888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 - Avg Training Loss: 1.9162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 5/7: 100%|██████████| 663/663 [01:28<00:00,  7.47it/s, loss=1.6611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 - Avg Training Loss: 1.6036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 6/7: 100%|██████████| 663/663 [01:29<00:00,  7.44it/s, loss=1.3162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 - Avg Training Loss: 1.4190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 7/7: 100%|██████████| 663/663 [01:29<00:00,  7.44it/s, loss=1.0920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 - Avg Training Loss: 1.3199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00,  9.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6387\n",
      "✅ Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 📌 Installer les packages nécessaires si besoin\n",
    "# !pip install transformers datasets torch scikit-learn pandas numpy tqdm accelerate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# ✅ Détection du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# ✅ Chargement des données\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# ✅ Nettoyage des textes\n",
    "def clean_text(text, max_length=256):  # ✅ Réduction à 256 tokens pour accélérer\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# ✅ Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# ✅ Filtrer les classes qui ont au moins 5 échantillons\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 5].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Réduction du dataset (max 25 000 exemples pour rapidité)\n",
    "max_samples = 25000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# ✅ Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Chargement du modèle DistilBERT (Rapide et performant)\n",
    "model_name = \"distilbert-base-uncased\"  # 📌 Plus rapide que DeBERTa\n",
    "print(f\"🚀 Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_id)).to(device)\n",
    "\n",
    "# ✅ Définition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):  # ✅ max_length optimisé\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ✅ Création des DataLoaders\n",
    "batch_size = 32  # ✅ Batch optimisé pour rapidité\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ✅ Optimisation : AdamW + Scheduler\n",
    "learning_rate = 2e-5  # ✅ LR ajusté pour rapidité\n",
    "epochs = 7  # ✅ Réduction des époques pour accélérer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=500, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "# ✅ Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ✅ Entraînement (Optimisé)\n",
    "print(\"🚀 Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # ✅ Mixed Precision Training\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # ✅ Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\"✅ Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2e42fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/airbuscomputervision/abifarah_nan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:93: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 1/10: 100%|██████████| 664/664 [01:54<00:00,  5.79it/s, loss=4.8026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Avg Training Loss: 5.4961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 2/10: 100%|██████████| 664/664 [01:46<00:00,  6.23it/s, loss=3.1351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Avg Training Loss: 3.8583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 3/10: 100%|██████████| 664/664 [01:47<00:00,  6.19it/s, loss=1.9742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Avg Training Loss: 2.5508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 4/10: 100%|██████████| 664/664 [01:46<00:00,  6.21it/s, loss=1.2819]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Avg Training Loss: 1.8354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 5/10: 100%|██████████| 664/664 [01:46<00:00,  6.21it/s, loss=1.5340]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Avg Training Loss: 1.4243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 6/10: 100%|██████████| 664/664 [01:47<00:00,  6.21it/s, loss=0.8184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Avg Training Loss: 1.1545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 7/10: 100%|██████████| 664/664 [01:47<00:00,  6.21it/s, loss=0.9530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Avg Training Loss: 0.9738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 8/10: 100%|██████████| 664/664 [01:47<00:00,  6.20it/s, loss=0.5812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Avg Training Loss: 0.8431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 9/10: 100%|██████████| 664/664 [01:47<00:00,  6.19it/s, loss=0.9668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Avg Training Loss: 0.7561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 10/10: 100%|██████████| 664/664 [01:47<00:00,  6.20it/s, loss=0.7178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Avg Training Loss: 0.7026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:07<00:00, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7364\n",
      "✅ Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 📌 Installer les packages nécessaires si besoin\n",
    "# !pip install transformers datasets torch scikit-learn pandas numpy tqdm accelerate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# ✅ Détection du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# ✅ Chargement des données\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# ✅ Nettoyage des textes\n",
    "def clean_text(text, max_length=256):  # ✅ Réduction à 256 tokens\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# ✅ Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# ✅ Filtrer les classes qui ont au moins 2 échantillons\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 2].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Réduction du dataset (max 25 000 exemples)\n",
    "max_samples = 25000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# ✅ Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Chargement du modèle RoBERTa (Meilleur que DistilBERT)\n",
    "model_name = \"roberta-base\"  # 📌 Plus puissant que DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_id)).to(device)\n",
    "\n",
    "# ✅ Activation du `gradient_checkpointing` (économie de mémoire)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# ✅ Définition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):  # ✅ max_length optimisé\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ✅ Création des DataLoaders\n",
    "batch_size = 32  # ✅ Optimisé pour GPU\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ✅ Optimisation : AdamW + Scheduler\n",
    "learning_rate = 2e-5  # ✅ LR plus bas pour une meilleure convergence\n",
    "epochs = 10  # ✅ Augmenté pour une meilleure précision\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=1000, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "# ✅ Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ✅ Entraînement (Optimisé)\n",
    "print(\"🚀 Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # ✅ Mixed Precision Training\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # ✅ Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\"✅ Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786d8fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/airbuscomputervision/abifarah_nan/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:93: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 1/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=4.7588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Avg Training Loss: 5.6270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 2/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=2.4141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Avg Training Loss: 3.2152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:31<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 3/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=0.9851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Avg Training Loss: 1.5922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 4/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=0.9995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Avg Training Loss: 1.0448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 5/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=0.4428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Avg Training Loss: 0.7092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 6/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=0.3385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Avg Training Loss: 0.5267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 7/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=0.4267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Avg Training Loss: 0.4022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 8/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=0.4626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Avg Training Loss: 0.3156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 9/10: 100%|██████████| 664/664 [08:56<00:00,  1.24it/s, loss=0.1822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Avg Training Loss: 0.2421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:31<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 10/10:  34%|███▍      | 226/664 [03:02<05:53,  1.24it/s, loss=0.3311]"
     ]
    }
   ],
   "source": [
    "# 📌 Installer les packages nécessaires si besoin\n",
    "# !pip install transformers datasets torch scikit-learn pandas numpy tqdm accelerate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# ✅ Détection du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# ✅ Chargement des données\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# ✅ Nettoyage des textes\n",
    "def clean_text(text, max_length=256):  # ✅ Réduction à 256 tokens\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# ✅ Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# ✅ Filtrer les classes qui ont au moins 2 échantillons\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 2].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Réduction du dataset (max 25 000 exemples)\n",
    "max_samples = 25000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# ✅ Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Chargement du modèle RoBERTa (Meilleur que DistilBERT)\n",
    "model_name = \"microsoft/deberta-v3-large\"  # 📌 Plus puissant que DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_id)).to(device)\n",
    "\n",
    "# ✅ Activation du `gradient_checkpointing` (économie de mémoire)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# ✅ Définition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):  # ✅ max_length optimisé\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ✅ Création des DataLoaders\n",
    "batch_size = 32  # ✅ Optimisé pour GPU\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ✅ Optimisation : AdamW + Scheduler\n",
    "learning_rate = 2e-5  # ✅ LR plus bas pour une meilleure convergence\n",
    "epochs = 10  # ✅ Augmenté pour une meilleure précision\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=1000, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "# ✅ Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ✅ Entraînement (Optimisé)\n",
    "print(\"🚀 Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # ✅ Mixed Precision Training\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # ✅ Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\"✅ Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fded11d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 5956/5956 [05:10<00:00, 19.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission file 'submission_bert.csv' generated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Process test data\n",
    "print(\"Processing test data...\")\n",
    "test_path = \"test_without_labels.csv\"\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Clean test data\n",
    "test_df[\"Cleaned_Text\"] = test_df[\"Text\"].apply(clean_text)\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, \n",
    "                                  max_length=max_length, return_tensors=\"pt\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "test_dataset = TestDataset(test_df[\"Cleaned_Text\"], tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Generate predictions\n",
    "model.eval()\n",
    "all_test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Generating predictions\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        # Get predictions\n",
    "        preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        all_test_preds.extend(preds)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "predicted_labels = [id_to_label[pred_id] for pred_id in all_test_preds]\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df.index + 1,  # Start IDs from 1\n",
    "    \"Label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_path = \"submission_bert.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"✅ Submission file '{submission_path}' generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d4127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading bert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6:  40%|███▉      | 2127/5344 [22:02<33:19,  1.61it/s, loss=1.7069]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 103\u001b[0m\n\u001b[1;32m    101\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    102\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 103\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    104\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Avg loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "import warnings\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load training data\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path)\n",
    "train_df_cleaned = train_df.dropna(subset=[\"Label\"])\n",
    "\n",
    "def clean_text(text, max_length=512):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df_cleaned[\"Cleaned_Text\"] = train_df_cleaned[\"Text\"].apply(lambda x: clean_text(x, max_length=256))\n",
    "train_df_cleaned = train_df_cleaned[train_df_cleaned[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "labels = train_df_cleaned[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df_cleaned[\"label_id\"] = train_df_cleaned[\"Label\"].map(label_to_id)\n",
    "\n",
    "# Balance dataset\n",
    "min_samples = 20\n",
    "valid_classes = train_df_cleaned[\"Label\"].value_counts()[lambda x: x >= min_samples].index\n",
    "train_df_filtered = train_df_cleaned[train_df_cleaned[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df_filtered[\"Cleaned_Text\"], \n",
    "    train_df_filtered[\"label_id\"], \n",
    "    test_size=0.1,\n",
    "    random_state=42, \n",
    "    stratify=train_df_filtered[\"label_id\"]\n",
    ")\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_name = \"bert-base-uncased\"  # Upgrade to full BERT\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label_to_id)).to(device)\n",
    "\n",
    "# Define Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, \n",
    "                                  max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Training setup\n",
    "learning_rate = 3e-5  # Slightly lower learning rate\n",
    "epochs = 6\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epochs\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "print(\"Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg loss: {train_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Final evaluation\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "final_accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Final validation accuracy: {final_accuracy:.4f}\")\n",
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e48cdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n",
      "Nombre total d'exemples après filtrage : 190087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:103: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/10: 100%|██████████| 531/531 [00:35<00:00, 14.88it/s, loss=2.8405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Avg Training Loss: 2.9818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:03<00:00, 19.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 2/10: 100%|██████████| 531/531 [00:36<00:00, 14.39it/s, loss=2.4766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Avg Training Loss: 2.5694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:03<00:00, 19.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 3/10: 100%|██████████| 531/531 [00:36<00:00, 14.40it/s, loss=2.4141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Avg Training Loss: 2.0705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:02<00:00, 19.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 4/10: 100%|██████████| 531/531 [00:36<00:00, 14.42it/s, loss=2.0706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Avg Training Loss: 1.6889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:02<00:00, 19.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 5/10: 100%|██████████| 531/531 [00:36<00:00, 14.41it/s, loss=1.5877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Avg Training Loss: 1.3878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:03<00:00, 19.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 6/10: 100%|██████████| 531/531 [00:37<00:00, 14.35it/s, loss=0.3924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Avg Training Loss: 1.1640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:02<00:00, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 7/10: 100%|██████████| 531/531 [00:36<00:00, 14.46it/s, loss=0.6785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Avg Training Loss: 1.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:03<00:00, 19.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 8/10: 100%|██████████| 531/531 [00:37<00:00, 14.30it/s, loss=0.6249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Avg Training Loss: 0.8795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:02<00:00, 19.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 9/10: 100%|██████████| 531/531 [00:36<00:00, 14.43it/s, loss=1.2419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Avg Training Loss: 0.7849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:02<00:00, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 10/10: 100%|██████████| 531/531 [00:36<00:00, 14.56it/s, loss=0.8906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Avg Training Loss: 0.7077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 59/59 [00:02<00:00, 20.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6403\n",
      "✅ Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # ✅ Accélération supplémentaire\n",
    "\n",
    "# ✅ Détection du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# ✅ Chargement des données\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# ✅ Nettoyage des textes\n",
    "def clean_text(text, max_length=128):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# ✅ Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# ✅ Filtrer les classes qui ont au moins 3 échantillons (moins strict)\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 3].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Vérifier la taille du dataset après filtrage\n",
    "print(f\"Nombre total d'exemples après filtrage : {len(train_df)}\")\n",
    "\n",
    "# ✅ Réduction du dataset (max 20 000 exemples pour vitesse)\n",
    "max_samples = 20000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# ✅ Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Chargement du modèle **correctement initialisé**\n",
    "model_name = \"distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label_to_id)\n",
    ").to(device)\n",
    "\n",
    "# ✅ Réinitialiser les poids de classification\n",
    "torch.nn.init.xavier_uniform_(model.classifier.out_proj.weight)\n",
    "torch.nn.init.zeros_(model.classifier.out_proj.bias)\n",
    "\n",
    "# ✅ Activation de `gradient_checkpointing`\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# ✅ Définition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ✅ Création des DataLoaders\n",
    "batch_size = 32  # ✅ Plus stable\n",
    "gradient_accumulation_steps = 2  # ✅ Accumulation pour économiser la mémoire GPU\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ✅ Optimisation : AdamW + Scheduler\n",
    "learning_rate = 2e-5  # ✅ Learning rate plus stable\n",
    "epochs = 10  # ✅ Légèrement augmenté pour convergence\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=1000, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "# ✅ Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ✅ Entraînement optimisé (1h max)\n",
    "print(\"🚀 Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / gradient_accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # ✅ Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\"✅ Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c928642c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/airbuscomputervision/abifarah_nan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:93: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 1/5: 100%|██████████| 664/664 [09:00<00:00,  1.23it/s, loss=4.0703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Avg Training Loss: 5.1601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:48<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 2/5: 100%|██████████| 664/664 [09:02<00:00,  1.22it/s, loss=2.1680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Avg Training Loss: 2.7878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:49<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 3/5: 100%|██████████| 664/664 [09:02<00:00,  1.22it/s, loss=1.4139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Avg Training Loss: 1.5903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:50<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 4/5: 100%|██████████| 664/664 [09:02<00:00,  1.22it/s, loss=1.0661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Avg Training Loss: 1.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:50<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 5/5: 100%|██████████| 664/664 [09:03<00:00,  1.22it/s, loss=1.3816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Avg Training Loss: 0.8968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 74/74 [00:49<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7246\n",
      "✅ Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 📌 Installer les packages nécessaires si besoin\n",
    "# !pip install transformers datasets torch scikit-learn pandas numpy tqdm accelerate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# ✅ Détection du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# ✅ Chargement des données\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# ✅ Nettoyage des textes\n",
    "def clean_text(text, max_length=256):  # ✅ Réduction à 256 tokens\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# ✅ Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# ✅ Filtrer les classes qui ont au moins 2 échantillons\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 2].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Réduction du dataset (max 25 000 exemples)\n",
    "max_samples = 25000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# ✅ Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Chargement du modèle RoBERTa (Meilleur que DistilBERT)\n",
    "model_name = \"roberta-large\"  # 📌 Plus puissant que DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_id)).to(device)\n",
    "\n",
    "# ✅ Activation du `gradient_checkpointing` (économie de mémoire)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# ✅ Définition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):  # ✅ max_length optimisé\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ✅ Création des DataLoaders\n",
    "batch_size = 32  # ✅ Optimisé pour GPU\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ✅ Optimisation : AdamW + Scheduler\n",
    "learning_rate = 2e-5  # ✅ LR plus bas pour une meilleure convergence\n",
    "epochs = 5  # ✅ Augmenté pour une meilleure précision\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=1000, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "# ✅ Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ✅ Entraînement (Optimisé)\n",
    "print(\"🚀 Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # ✅ Mixed Precision Training\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # ✅ Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\"✅ Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8286ef57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|█████████▉| 5949/5956 [35:26<00:02,  2.83it/s]"
     ]
    }
   ],
   "source": [
    "# Process test data\n",
    "print(\"Processing test data...\")\n",
    "test_path = \"test_without_labels.csv\"\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Clean test data\n",
    "test_df[\"Cleaned_Text\"] = test_df[\"Text\"].apply(clean_text)\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, \n",
    "                                  max_length=max_length, return_tensors=\"pt\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "test_dataset = TestDataset(test_df[\"Cleaned_Text\"], tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Generate predictions\n",
    "model.eval()\n",
    "all_test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Generating predictions\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        # Get predictions\n",
    "        preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        all_test_preds.extend(preds)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "predicted_labels = [id_to_label[pred_id] for pred_id in all_test_preds]\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df.index + 1,  # Start IDs from 1\n",
    "    \"Label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_path = \"submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"✅ Submission file '{submission_path}' generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad36065a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (628005962.py, line 81)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 81\u001b[0;36m\u001b[0m\n\u001b[0;31m    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\u001b[0m\n\u001b[0m                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "#%% Initial Setup\n",
    "!pip install transformers datasets torch scikit-learn pandas numpy tqdm accelerate wandb -q\n",
    "!pip install sentencepiece tensorboardx -q\n",
    "\n",
    "#%% Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#%% Configuration\n",
    "class Config:\n",
    "    # Model\n",
    "    MODEL_NAME = \"microsoft/deberta-v3-large\"  # Modèle state-of-the-art\n",
    "    MAX_LENGTH = 256  # Augmenter la longueur contextuelle\n",
    "    DROPOUT = 0.1\n",
    "    \n",
    "    # Training\n",
    "    EPOCHS = 5\n",
    "    BATCH_SIZE = 8  # Réduit pour gérer la mémoire\n",
    "    LR = 2e-5\n",
    "    WARMUP_STEPS = 100\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    GRAD_CLIP = 1.0\n",
    "    \n",
    "    # Data\n",
    "    SAMPLE_LIMIT = 50000  # Augmenter la taille de l'échantillon\n",
    "    MIN_SAMPLES_PER_CLASS = 20\n",
    "    TEST_SIZE = 0.1\n",
    "    \n",
    "    # Advanced\n",
    "    USE_CLASS_WEIGHTS = True\n",
    "    USE_FP16 = True  # Activation du mixed-precision\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "#%% Device Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#%% Advanced Text Cleaning\n",
    "def clean_text(text):\n",
    "    # Nettoyage approfondi\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # URLs\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)  # Hashtags/Mentions\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Ponctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:config.MAX_LENGTH]\n",
    "\n",
    "#%% Data Loading & Processing\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"Cleaned_Text\"] = df[\"Text\"].apply(clean_text)\n",
    "    df = df[df[\"Cleaned_Text\"].str.len() > 0]\n",
    "    return df\n",
    "\n",
    "# Load and preprocess data\n",
    "train_df = load_data(\"train_submission.csv\").dropna(subset=[\"Label\"])\n",
    "test_df = load_data(\"test_without_labels.csv\")\n",
    "\n",
    "# Gestion avancée des classes\n",
    "class_counts = train_df[\"Label\"].value_counts()\n",
    "valid_classes = class_counts[class_counts >= config.MIN_SAMPLES_PER_CLASS].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# Stratified sampling\n",
    "train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "    lambda x: x.sample(min(len(x), config.SAMPLE_LIMIT // len(valid_classes)), \n",
    ")\n",
    "\n",
    "# Label mapping\n",
    "label_to_id = {label: i for i, label in enumerate(valid_classes)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# Split stratifié\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"],\n",
    "    train_df[\"label_id\"],\n",
    "    test_size=config.TEST_SIZE,\n",
    "    stratify=train_df[\"label_id\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#%% Class Weight Calculation\n",
    "if config.USE_CLASS_WEIGHTS:\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "else:\n",
    "    class_weights = None\n",
    "\n",
    "#%% Tokenizer & Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.MODEL_NAME,\n",
    "    num_labels=len(label_to_id),\n",
    "    attention_probs_dropout_prob=config.DROPOUT,\n",
    "    hidden_dropout_prob=config.DROPOUT\n",
    ").to(device)\n",
    "\n",
    "#%% Dataset Class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=config.MAX_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "            \n",
    "        return item\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "\n",
    "#%% Data Loaders\n",
    "def create_loaders():\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.BATCH_SIZE*2,\n",
    "        shuffle=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_loader, val_loader = create_loaders()\n",
    "\n",
    "#%% Optimizer & Scheduler\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.LR,\n",
    "    weight_decay=config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "total_steps = len(train_loader) * config.EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.WARMUP_STEPS,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "#%% Mixed Precision Training\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.USE_FP16)\n",
    "\n",
    "#%% Training Loop\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device),\n",
    "            'labels': batch['labels'].to(device)\n",
    "        }\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=config.USE_FP16):\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            if config.USE_CLASS_WEIGHTS:\n",
    "                logits = outputs.logits\n",
    "                loss = torch.nn.functional.cross_entropy(\n",
    "                    logits.view(-1, model.config.num_labels),\n",
    "                    inputs['labels'].view(-1),\n",
    "                    weight=class_weights\n",
    "                )\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.GRAD_CLIP)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "#%% Evaluation Function\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "                'labels': batch['labels'].to(device)\n",
    "            }\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(inputs['labels'].cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(loader),\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "#%% Training with Early Stopping\n",
    "best_accuracy = 0\n",
    "patience_counter = 0\n",
    "patience = 2\n",
    "\n",
    "for epoch in range(config.EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.EPOCHS}\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss = train_epoch(model, train_loader)\n",
    "    \n",
    "    # Evaluation\n",
    "    val_metrics = evaluate(model, val_loader)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"Val Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Val F1: {val_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_metrics['accuracy'] > best_accuracy:\n",
    "        best_accuracy = val_metrics['accuracy']\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(\"New best model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "#%% Load Best Model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "#%% Final Evaluation\n",
    "final_metrics = evaluate(model, val_loader)\n",
    "print(\"\\nFinal Evaluation:\")\n",
    "print(f\"Accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {final_metrics['f1']:.4f}\")\n",
    "print(classification_report(y_val, all_preds, target_names=label_to_id.keys()))\n",
    "\n",
    "#%% Test Predictions\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=config.MAX_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "test_dataset = TestDataset(test_df[\"Cleaned_Text\"], tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE*2)\n",
    "\n",
    "#%% Generate Predictions\n",
    "model.eval()\n",
    "test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "        test_preds.extend(preds)\n",
    "\n",
    "#%% Create Submission\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df.index + 1,\n",
    "    \"Label\": [id_to_label[pred] for pred in test_preds]\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_final.csv\", index=False)\n",
    "print(\"Submission file created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3249f1bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/airbuscomputervision/abifarah_nan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading and preprocessing data...\n",
      "Number of classes: 374\n",
      "Sample distribution: Label\n",
      "tgk    1500\n",
      "hbs    1000\n",
      "mon    1000\n",
      "crh    1000\n",
      "som    1000\n",
      "Name: count, dtype: int64...\n",
      "Loading xlm-roberta-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 1100/1100 [02:18<00:00,  7.92it/s, loss=5.3197]\n",
      "Validating: 100%|██████████| 123/123 [00:03<00:00, 31.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 - Time: 142.77s\n",
      "Train Loss: 5.7318, Val Loss: 5.0986\n",
      "Validation Accuracy: 0.2757\n",
      "✓ New best model saved with accuracy: 0.2757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 1100/1100 [02:24<00:00,  7.59it/s, loss=4.6401]\n",
      "Validating: 100%|██████████| 123/123 [00:03<00:00, 33.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4 - Time: 148.63s\n",
      "Train Loss: 4.8803, Val Loss: 4.2738\n",
      "Validation Accuracy: 0.3867\n",
      "✓ New best model saved with accuracy: 0.3867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 1100/1100 [02:23<00:00,  7.68it/s, loss=4.7164]\n",
      "Validating: 100%|██████████| 123/123 [00:03<00:00, 33.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4 - Time: 146.94s\n",
      "Train Loss: 4.2757, Val Loss: 3.8597\n",
      "Validation Accuracy: 0.4440\n",
      "✓ New best model saved with accuracy: 0.4440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|██████████| 1100/1100 [02:24<00:00,  7.61it/s, loss=4.1735]\n",
      "Validating: 100%|██████████| 123/123 [00:03<00:00, 34.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4 - Time: 148.15s\n",
      "Train Loss: 3.9681, Val Loss: 3.7116\n",
      "Validation Accuracy: 0.4588\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 219\u001b[0m\n\u001b[1;32m    217\u001b[0m     no_improve_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# Save best model\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_xlm_roberta_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ New best model saved with accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:943\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    940\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 943\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    944\u001b[0m         _save(\n\u001b[1;32m    945\u001b[0m             obj,\n\u001b[1;32m    946\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    949\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    950\u001b[0m         )\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:784\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, get_linear_schedule_with_warmup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced text cleaning function\n",
    "def advanced_clean_text(text, max_length=512):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Normalize Unicode characters\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Keep some punctuation that might be language-specific\n",
    "    text = re.sub(r'[^\\w\\s\\'\"-]', '', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text[:max_length]\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load training data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path)\n",
    "\n",
    "# Remove missing labels and clean data\n",
    "train_df_cleaned = train_df.dropna(subset=[\"Label\"])\n",
    "train_df_cleaned[\"Cleaned_Text\"] = train_df_cleaned[\"Text\"].apply(advanced_clean_text)\n",
    "\n",
    "# Remove empty texts\n",
    "train_df_cleaned = train_df_cleaned[train_df_cleaned[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# Create label mapping\n",
    "labels = train_df_cleaned[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "# Convert labels to IDs\n",
    "train_df_cleaned[\"label_id\"] = train_df_cleaned[\"Label\"].map(label_to_id)\n",
    "\n",
    "# Check and filter class distribution\n",
    "class_counts = train_df_cleaned[\"Label\"].value_counts()\n",
    "min_samples = 10\n",
    "valid_classes = class_counts[class_counts >= min_samples].index\n",
    "train_df_filtered = train_df_cleaned[train_df_cleaned[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "print(f\"Number of classes: {len(valid_classes)}\")\n",
    "print(f\"Sample distribution: {class_counts[valid_classes][:5]}...\")\n",
    "\n",
    "# Stratified sampling\n",
    "max_samples = 20000\n",
    "if len(train_df_filtered) > max_samples:\n",
    "    train_df_filtered = train_df_filtered.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(valid_classes)), random_state=42)\n",
    "    )\n",
    "\n",
    "# Reset index to avoid potential indexing issues\n",
    "train_df_filtered = train_df_filtered.reset_index(drop=True)\n",
    "\n",
    "# Split data with stratification\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df_filtered[\"Cleaned_Text\"], \n",
    "    train_df_filtered[\"label_id\"], \n",
    "    test_size=0.1,\n",
    "    random_state=42, \n",
    "    stratify=train_df_filtered[\"label_id\"]\n",
    ")\n",
    "\n",
    "# Use XLM-RoBERTa model\n",
    "model_name = \"xlm-roberta-base\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_to_id),\n",
    "    ignore_mismatched_sizes=True  # Add this to handle potential size mismatches\n",
    ").to(device)\n",
    "\n",
    "# Custom Dataset with safer data handling\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class SafeTextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.encodings = self._prepare_encodings()\n",
    "    \n",
    "    def _prepare_encodings(self):\n",
    "        return self.tokenizer(\n",
    "            self.texts, \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SafeTextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = SafeTextDataset(X_val, y_val, tokenizer)\n",
    "\n",
    "# Improved training configuration with a suitable batch size for XLM-RoBERTa\n",
    "batch_size = 16  # Can be reduced to 8 if memory issues occur\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Advanced training setup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Optimized hyperparameters for XLM-RoBERTa\n",
    "learning_rate = 1e-5  # Slightly lower learning rate for XLM-RoBERTa\n",
    "epochs = 4  # Increased epochs for better performance\n",
    "weight_decay = 0.01\n",
    "warmup_ratio = 0.1\n",
    "\n",
    "# Prepare optimizer and schedule\n",
    "optimizer = AdamW(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate, \n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=int(warmup_ratio * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop with early stopping and accuracy tracking\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "best_val_accuracy = 0\n",
    "patience = 3  # Increased patience\n",
    "no_improve_epochs = 0\n",
    "training_start_time = time.time()\n",
    "\n",
    "print(\"Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            \n",
    "            val_loss += outputs.loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Time: {epoch_time:.2f}s\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        no_improve_epochs = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'best_xlm_roberta_model.pth')\n",
    "        print(f\"✓ New best model saved with accuracy: {val_accuracy:.4f}\")\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f\"No improvement for {no_improve_epochs} epochs\")\n",
    "        \n",
    "    if no_improve_epochs >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "total_training_time = time.time() - training_start_time\n",
    "print(f\"Total training time: {total_training_time:.2f} seconds\")\n",
    "\n",
    "# Load best model for final evaluation\n",
    "model.load_state_dict(torch.load('best_xlm_roberta_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Final evaluation\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Final evaluation\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "final_accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Final validation accuracy: {final_accuracy:.4f}\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Process test data\n",
    "print(\"Processing test data...\")\n",
    "test_path = \"test_without_labels.csv\"\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Clean test data\n",
    "test_df[\"Cleaned_Text\"] = test_df[\"Text\"].apply(advanced_clean_text)\n",
    "\n",
    "# Create test dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(\n",
    "            texts.tolist(), \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "test_dataset = TestDataset(test_df[\"Cleaned_Text\"], tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Generate predictions\n",
    "model.eval()\n",
    "all_test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Generating predictions\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        all_test_preds.extend(preds)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "predicted_labels = [id_to_label[pred_id] for pred_id in all_test_preds]\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df.index + 1,\n",
    "    \"Label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_path = \"submission_file.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"✅ Submission file '{submission_path}' generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa87d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 Installer les packages nécessaires si besoin\n",
    "# !pip install transformers datasets torch scikit-learn pandas numpy tqdm accelerate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# ✅ Détection du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# ✅ Chargement des données\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# ✅ Nettoyage des textes\n",
    "def clean_text(text, max_length=256):  # ✅ Réduction à 256 tokens\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# ✅ Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# ✅ Filtrer les classes qui ont au moins 2 échantillons\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 2].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# ✅ Réduction du dataset (max 25 000 exemples)\n",
    "max_samples = 25000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# ✅ Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Chargement du modèle RoBERTa (Meilleur que DistilBERT)\n",
    "model_name = \"roberta-large\"  # 📌 Plus puissant que DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_id)).to(device)\n",
    "\n",
    "# ✅ Activation du `gradient_checkpointing` (économie de mémoire)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# ✅ Définition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):  # ✅ max_length optimisé\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ✅ Création des DataLoaders\n",
    "batch_size = 32  # ✅ Optimisé pour GPU\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ✅ Optimisation : AdamW + Scheduler\n",
    "learning_rate = 2e-5  # ✅ LR plus bas pour une meilleure convergence\n",
    "epochs = 5  # ✅ Augmenté pour une meilleure précision\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=1000, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "# ✅ Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ✅ Entraînement (Optimisé)\n",
    "print(\"🚀 Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # ✅ Mixed Precision Training\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # ✅ Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\"✅ Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1645013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d804a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41887f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de64a46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98252bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dfd825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mon_env)",
   "language": "python",
   "name": "mon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
