{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763e1485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\r\n",
      "Built on Wed_Jun__8_16:49:14_PDT_2022\r\n",
      "Cuda compilation tools, release 11.7, V11.7.99\r\n",
      "Build cuda_11.7.r11.7/compiler.31442593_0\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d8542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/airbuscomputervision/abifarah_nan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Using device: cuda\n",
      "ðŸš€ Loading distilbert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:92: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 1/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 663/663 [01:22<00:00,  7.99it/s, loss=4.3890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 - Avg Training Loss: 5.4645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 2/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 663/663 [01:26<00:00,  7.66it/s, loss=2.7561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 - Avg Training Loss: 3.5443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 3/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 663/663 [01:27<00:00,  7.57it/s, loss=1.8300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 - Avg Training Loss: 2.4533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 4/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 663/663 [01:28<00:00,  7.51it/s, loss=1.8888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 - Avg Training Loss: 1.9162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 5/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 663/663 [01:28<00:00,  7.47it/s, loss=1.6611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 - Avg Training Loss: 1.6036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 6/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 663/663 [01:29<00:00,  7.44it/s, loss=1.3162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 - Avg Training Loss: 1.4190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/7:   0%|          | 0/663 [00:00<?, ?it/s]/tmp/abifarah_nan-98193/ipykernel_63641/1981024072.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 7/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 663/663 [01:29<00:00,  7.44it/s, loss=1.0920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 - Avg Training Loss: 1.3199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00,  9.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6387\n",
      "âœ… Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ Installer les packages nÃ©cessaires si besoin\n",
    "# !pip install transformers datasets torch scikit-learn pandas numpy tqdm accelerate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# âœ… DÃ©tection du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Using device: {device}\")\n",
    "\n",
    "# âœ… Chargement des donnÃ©es\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# âœ… Nettoyage des textes\n",
    "def clean_text(text, max_length=256):  # âœ… RÃ©duction Ã  256 tokens pour accÃ©lÃ©rer\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# âœ… Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# âœ… Filtrer les classes qui ont au moins 5 Ã©chantillons\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 5].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# âœ… RÃ©duction du dataset (max 25 000 exemples pour rapiditÃ©)\n",
    "max_samples = 25000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# âœ… Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# âœ… Chargement du modÃ¨le DistilBERT (Rapide et performant)\n",
    "model_name = \"distilbert-base-uncased\"  # ðŸ“Œ Plus rapide que DeBERTa\n",
    "print(f\"ðŸš€ Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_id)).to(device)\n",
    "\n",
    "# âœ… DÃ©finition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):  # âœ… max_length optimisÃ©\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# âœ… CrÃ©ation des DataLoaders\n",
    "batch_size = 32  # âœ… Batch optimisÃ© pour rapiditÃ©\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# âœ… Optimisation : AdamW + Scheduler\n",
    "learning_rate = 2e-5  # âœ… LR ajustÃ© pour rapiditÃ©\n",
    "epochs = 7  # âœ… RÃ©duction des Ã©poques pour accÃ©lÃ©rer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=500, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "# âœ… Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# âœ… EntraÃ®nement (OptimisÃ©)\n",
    "print(\"ðŸš€ Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # âœ… Mixed Precision Training\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # âœ… Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\"âœ… Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2e42fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/airbuscomputervision/abifarah_nan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:93: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [01:54<00:00,  5.79it/s, loss=4.8026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Avg Training Loss: 5.4961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [01:46<00:00,  6.23it/s, loss=3.1351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Avg Training Loss: 3.8583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [01:47<00:00,  6.19it/s, loss=1.9742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Avg Training Loss: 2.5508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [01:46<00:00,  6.21it/s, loss=1.2819]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Avg Training Loss: 1.8354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [01:46<00:00,  6.21it/s, loss=1.5340]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Avg Training Loss: 1.4243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [01:47<00:00,  6.21it/s, loss=0.8184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Avg Training Loss: 1.1545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [01:47<00:00,  6.21it/s, loss=0.9530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Avg Training Loss: 0.9738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [01:47<00:00,  6.20it/s, loss=0.5812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Avg Training Loss: 0.8431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [01:47<00:00,  6.19it/s, loss=0.9668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Avg Training Loss: 0.7561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/3085072444.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [01:47<00:00,  6.20it/s, loss=0.7178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Avg Training Loss: 0.7026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:07<00:00, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7364\n",
      "âœ… Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ Installer les packages nÃ©cessaires si besoin\n",
    "# !pip install transformers datasets torch scikit-learn pandas numpy tqdm accelerate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# âœ… DÃ©tection du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Using device: {device}\")\n",
    "\n",
    "# âœ… Chargement des donnÃ©es\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# âœ… Nettoyage des textes\n",
    "def clean_text(text, max_length=256):  # âœ… RÃ©duction Ã  256 tokens\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# âœ… Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# âœ… Filtrer les classes qui ont au moins 2 Ã©chantillons\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 2].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# âœ… RÃ©duction du dataset (max 25 000 exemples)\n",
    "max_samples = 25000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# âœ… Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# âœ… Chargement du modÃ¨le RoBERTa (Meilleur que DistilBERT)\n",
    "model_name = \"roberta-base\"  # ðŸ“Œ Plus puissant que DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_id)).to(device)\n",
    "\n",
    "# âœ… Activation du `gradient_checkpointing` (Ã©conomie de mÃ©moire)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# âœ… DÃ©finition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):  # âœ… max_length optimisÃ©\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# âœ… CrÃ©ation des DataLoaders\n",
    "batch_size = 32  # âœ… OptimisÃ© pour GPU\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# âœ… Optimisation : AdamW + Scheduler\n",
    "learning_rate = 2e-5  # âœ… LR plus bas pour une meilleure convergence\n",
    "epochs = 10  # âœ… AugmentÃ© pour une meilleure prÃ©cision\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=1000, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "# âœ… Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# âœ… EntraÃ®nement (OptimisÃ©)\n",
    "print(\"ðŸš€ Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # âœ… Mixed Precision Training\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # âœ… Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\"âœ… Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786d8fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/airbuscomputervision/abifarah_nan/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:93: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [08:56<00:00,  1.24it/s, loss=4.7588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Avg Training Loss: 5.6270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [08:56<00:00,  1.24it/s, loss=2.4141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Avg Training Loss: 3.2152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:31<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [08:56<00:00,  1.24it/s, loss=0.9851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Avg Training Loss: 1.5922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [08:56<00:00,  1.24it/s, loss=0.9995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Avg Training Loss: 1.0448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [08:56<00:00,  1.24it/s, loss=0.4428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Avg Training Loss: 0.7092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [08:56<00:00,  1.24it/s, loss=0.3385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Avg Training Loss: 0.5267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [08:56<00:00,  1.24it/s, loss=0.4267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Avg Training Loss: 0.4022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [08:56<00:00,  1.24it/s, loss=0.4626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Avg Training Loss: 0.3156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:32<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [08:56<00:00,  1.24it/s, loss=0.1822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Avg Training Loss: 0.2421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:31<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-98123/ipykernel_3775977/1810347954.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 10/10:  34%|â–ˆâ–ˆâ–ˆâ–      | 226/664 [03:02<05:53,  1.24it/s, loss=0.3311]"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ Installer les packages nÃ©cessaires si besoin\n",
    "# !pip install transformers datasets torch scikit-learn pandas numpy tqdm accelerate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# âœ… DÃ©tection du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Using device: {device}\")\n",
    "\n",
    "# âœ… Chargement des donnÃ©es\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# âœ… Nettoyage des textes\n",
    "def clean_text(text, max_length=256):  # âœ… RÃ©duction Ã  256 tokens\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# âœ… Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# âœ… Filtrer les classes qui ont au moins 2 Ã©chantillons\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 2].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# âœ… RÃ©duction du dataset (max 25 000 exemples)\n",
    "max_samples = 25000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# âœ… Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# âœ… Chargement du modÃ¨le RoBERTa (Meilleur que DistilBERT)\n",
    "model_name = \"microsoft/deberta-v3-large\"  # ðŸ“Œ Plus puissant que DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_id)).to(device)\n",
    "\n",
    "# âœ… Activation du `gradient_checkpointing` (Ã©conomie de mÃ©moire)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# âœ… DÃ©finition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):  # âœ… max_length optimisÃ©\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# âœ… CrÃ©ation des DataLoaders\n",
    "batch_size = 32  # âœ… OptimisÃ© pour GPU\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# âœ… Optimisation : AdamW + Scheduler\n",
    "learning_rate = 2e-5  # âœ… LR plus bas pour une meilleure convergence\n",
    "epochs = 10  # âœ… AugmentÃ© pour une meilleure prÃ©cision\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=1000, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "# âœ… Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# âœ… EntraÃ®nement (OptimisÃ©)\n",
    "print(\"ðŸš€ Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # âœ… Mixed Precision Training\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # âœ… Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\"âœ… Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fded11d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5956/5956 [05:10<00:00, 19.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Submission file 'submission_bert.csv' generated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Process test data\n",
    "print(\"Processing test data...\")\n",
    "test_path = \"test_without_labels.csv\"\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Clean test data\n",
    "test_df[\"Cleaned_Text\"] = test_df[\"Text\"].apply(clean_text)\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, \n",
    "                                  max_length=max_length, return_tensors=\"pt\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "test_dataset = TestDataset(test_df[\"Cleaned_Text\"], tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Generate predictions\n",
    "model.eval()\n",
    "all_test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Generating predictions\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        # Get predictions\n",
    "        preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        all_test_preds.extend(preds)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "predicted_labels = [id_to_label[pred_id] for pred_id in all_test_preds]\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df.index + 1,  # Start IDs from 1\n",
    "    \"Label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_path = \"submission_bert.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"âœ… Submission file '{submission_path}' generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e48cdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Using device: cuda\n",
      "Nombre total d'exemples aprÃ¨s filtrage : 190087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:103: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 531/531 [00:35<00:00, 14.88it/s, loss=2.8405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Avg Training Loss: 2.9818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:03<00:00, 19.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 531/531 [00:36<00:00, 14.39it/s, loss=2.4766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Avg Training Loss: 2.5694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:03<00:00, 19.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 531/531 [00:36<00:00, 14.40it/s, loss=2.4141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Avg Training Loss: 2.0705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:02<00:00, 19.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 531/531 [00:36<00:00, 14.42it/s, loss=2.0706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Avg Training Loss: 1.6889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:02<00:00, 19.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 531/531 [00:36<00:00, 14.41it/s, loss=1.5877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Avg Training Loss: 1.3878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:03<00:00, 19.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 531/531 [00:37<00:00, 14.35it/s, loss=0.3924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Avg Training Loss: 1.1640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:02<00:00, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 531/531 [00:36<00:00, 14.46it/s, loss=0.6785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Avg Training Loss: 1.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:03<00:00, 19.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 531/531 [00:37<00:00, 14.30it/s, loss=0.6249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Avg Training Loss: 0.8795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:02<00:00, 19.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 531/531 [00:36<00:00, 14.43it/s, loss=1.2419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Avg Training Loss: 0.7849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:02<00:00, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   0%|          | 0/531 [00:00<?, ?it/s]/tmp/abifarah_nan-97958/ipykernel_25717/1222409659.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 531/531 [00:36<00:00, 14.56it/s, loss=0.8906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Avg Training Loss: 0.7077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:02<00:00, 20.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6403\n",
      "âœ… Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # âœ… AccÃ©lÃ©ration supplÃ©mentaire\n",
    "\n",
    "# âœ… DÃ©tection du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Using device: {device}\")\n",
    "\n",
    "# âœ… Chargement des donnÃ©es\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# âœ… Nettoyage des textes\n",
    "def clean_text(text, max_length=128):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# âœ… Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# âœ… Filtrer les classes qui ont au moins 3 Ã©chantillons (moins strict)\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 3].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# âœ… VÃ©rifier la taille du dataset aprÃ¨s filtrage\n",
    "print(f\"Nombre total d'exemples aprÃ¨s filtrage : {len(train_df)}\")\n",
    "\n",
    "# âœ… RÃ©duction du dataset (max 20 000 exemples pour vitesse)\n",
    "max_samples = 20000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# âœ… Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# âœ… Chargement du modÃ¨le **correctement initialisÃ©**\n",
    "model_name = \"distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label_to_id)\n",
    ").to(device)\n",
    "\n",
    "# âœ… RÃ©initialiser les poids de classification\n",
    "torch.nn.init.xavier_uniform_(model.classifier.out_proj.weight)\n",
    "torch.nn.init.zeros_(model.classifier.out_proj.bias)\n",
    "\n",
    "# âœ… Activation de `gradient_checkpointing`\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# âœ… DÃ©finition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# âœ… CrÃ©ation des DataLoaders\n",
    "batch_size = 32  # âœ… Plus stable\n",
    "gradient_accumulation_steps = 2  # âœ… Accumulation pour Ã©conomiser la mÃ©moire GPU\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# âœ… Optimisation : AdamW + Scheduler\n",
    "learning_rate = 2e-5  # âœ… Learning rate plus stable\n",
    "epochs = 10  # âœ… LÃ©gÃ¨rement augmentÃ© pour convergence\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=1000, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "# âœ… Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# âœ… EntraÃ®nement optimisÃ© (1h max)\n",
    "print(\"ðŸš€ Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / gradient_accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # âœ… Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\"âœ… Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c928642c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/airbuscomputervision/abifarah_nan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:93: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [09:00<00:00,  1.23it/s, loss=4.0703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Avg Training Loss: 5.1601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:48<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [09:02<00:00,  1.22it/s, loss=2.1680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Avg Training Loss: 2.7878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:49<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [09:02<00:00,  1.22it/s, loss=1.4139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Avg Training Loss: 1.5903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:50<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [09:02<00:00,  1.22it/s, loss=1.0661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Avg Training Loss: 1.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:50<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:   0%|          | 0/664 [00:00<?, ?it/s]/tmp/abifarah_nan-97976/ipykernel_26281/177140864.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # âœ… Mixed Precision Training\n",
      "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [09:03<00:00,  1.22it/s, loss=1.3816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Avg Training Loss: 0.8968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:49<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7246\n",
      "âœ… Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ Installer les packages nÃ©cessaires si besoin\n",
    "# !pip install transformers datasets torch scikit-learn pandas numpy tqdm accelerate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# âœ… DÃ©tection du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Using device: {device}\")\n",
    "\n",
    "# âœ… Chargement des donnÃ©es\n",
    "train_path = \"train_submission.csv\"\n",
    "train_df = pd.read_csv(train_path).dropna(subset=[\"Label\"])\n",
    "\n",
    "# âœ… Nettoyage des textes\n",
    "def clean_text(text, max_length=256):  # âœ… RÃ©duction Ã  256 tokens\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text[:max_length]\n",
    "\n",
    "train_df[\"Cleaned_Text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "train_df = train_df[train_df[\"Cleaned_Text\"].str.len() > 0]\n",
    "\n",
    "# âœ… Encodage des labels\n",
    "labels = train_df[\"Label\"].unique().tolist()\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "train_df[\"label_id\"] = train_df[\"Label\"].map(label_to_id)\n",
    "\n",
    "# âœ… Filtrer les classes qui ont au moins 2 Ã©chantillons\n",
    "valid_classes = train_df[\"Label\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 2].index\n",
    "train_df = train_df[train_df[\"Label\"].isin(valid_classes)]\n",
    "\n",
    "# âœ… RÃ©duction du dataset (max 25 000 exemples)\n",
    "max_samples = 25000\n",
    "if len(train_df) > max_samples:\n",
    "    train_df = train_df.groupby(\"Label\", group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples // len(labels)), random_state=42)\n",
    "    )\n",
    "\n",
    "# âœ… Division Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"Cleaned_Text\"], train_df[\"label_id\"], test_size=0.1, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# âœ… Chargement du modÃ¨le RoBERTa (Meilleur que DistilBERT)\n",
    "model_name = \"roberta-large\"  # ðŸ“Œ Plus puissant que DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_id)).to(device)\n",
    "\n",
    "# âœ… Activation du `gradient_checkpointing` (Ã©conomie de mÃ©moire)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# âœ… DÃ©finition du Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):  # âœ… max_length optimisÃ©\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# âœ… CrÃ©ation des DataLoaders\n",
    "batch_size = 32  # âœ… OptimisÃ© pour GPU\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# âœ… Optimisation : AdamW + Scheduler\n",
    "learning_rate = 2e-5  # âœ… LR plus bas pour une meilleure convergence\n",
    "epochs = 5  # âœ… AugmentÃ© pour une meilleure prÃ©cision\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=1000, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "# âœ… Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# âœ… EntraÃ®nement (OptimisÃ©)\n",
    "print(\"ðŸš€ Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # âœ… Mixed Precision Training\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # âœ… Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "print(\"âœ… Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8286ef57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5949/5956 [35:26<00:02,  2.83it/s]"
     ]
    }
   ],
   "source": [
    "# Process test data\n",
    "print(\"Processing test data...\")\n",
    "test_path = \"test_without_labels.csv\"\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Clean test data\n",
    "test_df[\"Cleaned_Text\"] = test_df[\"Text\"].apply(clean_text)\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, \n",
    "                                  max_length=max_length, return_tensors=\"pt\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "test_dataset = TestDataset(test_df[\"Cleaned_Text\"], tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Generate predictions\n",
    "model.eval()\n",
    "all_test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Generating predictions\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        # Get predictions\n",
    "        preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        all_test_preds.extend(preds)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "predicted_labels = [id_to_label[pred_id] for pred_id in all_test_preds]\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df.index + 1,  # Start IDs from 1\n",
    "    \"Label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_path = \"submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"âœ… Submission file '{submission_path}' generated successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mon_env)",
   "language": "python",
   "name": "mon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
